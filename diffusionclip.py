from pynvml import *import timefrom glob import globfrom tqdm import tqdmimport osimport numpy as npimport cv2from PIL import Imageimport torchfrom torch import nnimport torchvision.utils as tvuimport torchfrom models.ddpm.diffusion import DDPMfrom models.ddpm.diffusion_lora import DDPM_lora, load_weight_lorafrom models.improved_ddpm.script_util import i_DDPMfrom utils.text_dic import SRC_TRG_TXT_DICfrom utils.diffusion_utils import get_beta_schedule, denoising_stepfrom utils.wandb_writer import WanDBWriterfrom losses import id_lossfrom losses.clip_loss import CLIPLossfrom datasets.data_utils import get_dataset, get_dataloaderfrom configs.paths_config import DATASET_PATHS, MODEL_PATHS, HYBRID_MODEL_PATHS, HYBRID_CONFIGfrom datasets.imagenet_dic import IMAGENET_DICfrom utils.align_utils import run_alignmentfrom freeze import *import loralib as loraDEFAULT_DDPM_SIZE = 113673219class DiffusionCLIP(object):    def __init__(self, args, config, device=None):        self.args = args        self.config = config        self.logger = None        # set_random_seed(self.args.seed)        # print(type(config))        # print(config)        if device is None:            device = torch.device(                "cuda:0") if torch.cuda.is_available() else torch.device("cpu")        self.device = torch.device(device)        self.model_var_type = config.model.var_type        betas = get_beta_schedule(            beta_start=config.diffusion.beta_start,            beta_end=config.diffusion.beta_end,            num_diffusion_timesteps=config.diffusion.num_diffusion_timesteps        )        self.betas = torch.from_numpy(betas).float()        self.num_timesteps = betas.shape[0]        self.is_first = True        self.is_first_train = True        alphas = 1.0 - betas        alphas_cumprod = np.cumprod(alphas, axis=0)        self.alphas_cumprod = alphas_cumprod        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])        posterior_variance = betas * \                             (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)        if self.model_var_type == "fixedlarge":            self.logvar = np.log(np.append(posterior_variance[1], betas[1:]))        elif self.model_var_type == 'fixedsmall':            self.logvar = np.log(np.maximum(posterior_variance, 1e-20))        self.betas = self.betas.to(self.device)        self.logvar = torch.tensor(self.logvar).float().to(self.device)        if self.args.edit_attr is None:            self.src_txts = self.args.src_txts            self.trg_txts = self.args.trg_txts        else:            self.src_txts = SRC_TRG_TXT_DIC[self.args.edit_attr][0]            self.trg_txts = SRC_TRG_TXT_DIC[self.args.edit_attr][1]        # Some configurations        self._conf_model()        self._conf_opt()        self._conf_loss()        self._conf_seqs()        # print('checking requires grad init')        # for param in self.model.parameters():        #     print(param.requires_grad)        # print('\n\n')    # Forward propagation of diffusion model    # ----------------------------------------------------------------------------------    def apply_diffusion(self, x, seq_prev, seq_next, eta=0.0, sample_type='ddim', process='undefined',                        is_one_step=False, simple=False,                        is_grad=False):        if simple:            t0 = self.args.t_0            l1 = self.alphas_cumprod[t0]            x = x * l1 ** 0.5 + (1 - l1) ** 0.5 * torch.randn_like(x)            return x        n = len(x)        with torch.set_grad_enabled(is_grad):            for it, (i, j) in enumerate(zip(seq_prev, seq_next)):                t = (torch.ones(n) * i).to(self.device)                t_prev = (torch.ones(n) * j).to(self.device)                x, x0 = denoising_step(x, t=t, t_next=t_prev,                                       models=self.model,                                       logvars=self.logvar,                                       sampling_type=sample_type,                                       b=self.betas,                                       eta=eta,                                       out_x0_t=True,                                       learn_sigma=self.learn_sigma)                if is_one_step:                    return x0        return x    # ----------------------------------------------------------------------------------    # Slava's code    def edit_one_image(self):        # ----------- Data -----------#        n = self.args.bs_test        if self.args.align_face and self.config.data.dataset in ["FFHQ", "CelebA_HQ"]:            try:                img = run_alignment(self.args.img_path, output_size=self.config.data.image_size)            except:                img = Image.open(self.args.img_path).convert("RGB")        else:            img = Image.open(self.args.img_path).convert("RGB")        img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        # print(self.args.img_path)        # self.logger.set_step(0)        # self.logger.add_image('photo', img)        # self.logger.set_step(self.args.n_train_img)        img = np.array(img)/255        img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        img = img.to(self.config.device)        tvu.save_image(img, os.path.join(self.args.image_folder, f'0_orig.png'))        # print(self.args.image_folder, 'image folder')        # print(os.path.join(self.args.image_folder, f'0_orig.png'))        x0 = (img - 0.5) * 2.        # ----------- Models -----------#        if self.config.data.dataset == "LSUN":            if self.config.data.category == "bedroom":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt"            elif self.config.data.category == "church_outdoor":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt"        elif self.config.data.dataset == "CelebA_HQ":            url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt"        elif self.config.data.dataset in ["FFHQ", "AFHQ", "IMAGENET"]:            pass        else:            raise ValueError        models = []        if self.args.hybrid_noise:            model_paths = [None] + HYBRID_MODEL_PATHS        else:            # model_paths = [None, self.args.model_path]            model_paths = [None, self.args.save_name]            # TODO: тут бага, веса лоры не подгружаются. По-хорошему поменять этот код нахуй            # TODO: тут баги нет, нам нужно 2 модели для предсказания. Надо просто сделать так, чтобы            # в случае лоры подгружалась вся лора            # TODO: лучше конечно сделать так, чтобы лора подгружалась из self.model, а не сейва торча            # (да и не только лора)        for model_path in model_paths:            if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:                if self.args.lora and model_path:                    model_i = DDPM_lora(self.config, self.args.lora_rank, self.args.lora_alpha)                    # print('lora in gen')                else:                    model_i = DDPM(self.config)                if model_path:                    ckpt = torch.load(model_path)                    # print('loaded', model_path)                else:                    ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)                learn_sigma = False            elif self.config.data.dataset in ["FFHQ", "AFHQ", "IMAGENET"]:                model_i = i_DDPM(self.config.data.dataset)                if model_path:                    ckpt = torch.load(model_path)                else:                    ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])                learn_sigma = True            else:                print('Not implemented dataset')                raise ValueError            # if self.args.lora:            #     load_weight_lora(model_i, ckpt)            # else:            #     model_i.load_state_dict(ckpt)            model_i.load_state_dict(ckpt)            model_i.to(self.device)            # model_i = torch.nn.DataParallel(model_i)            model_i.eval()            # print(f"{model_path} is loaded.")            models.append(model_i)        with torch.no_grad():            #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#            if self.args.deterministic_inv:                x_lat_path = os.path.join(self.args.image_folder, f'x_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}.pth')                # print(x_lat_path, os.path.exists(x_lat_path))                if not os.path.exists(x_lat_path):                    seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0                    seq_inv = [int(s) for s in list(seq_inv)]                    seq_inv_next = [-1] + list(seq_inv[:-1])                    x = x0.clone()                    with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:                        for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):                            t = (torch.ones(n) * i).to(self.device)                            t_prev = (torch.ones(n) * j).to(self.device)                            x = denoising_step(x, t=t, t_next=t_prev, models=models,                                               logvars=self.logvar,                                               sampling_type='ddim',                                               b=self.betas,                                               eta=0,                                               learn_sigma=learn_sigma,                                               ratio=0,                                               )                            progress_bar.update(1)                        x_lat = x.clone()                        torch.save(x_lat, x_lat_path)                else:                    # print('Latent exists.', x_lat_path)                    x_lat = torch.load(x_lat_path)            # ----------- Generative Process -----------#            # print(f"Sampling type: {self.args.sample_type.upper()} with eta {self.args.eta}, "            #       f" Steps: {self.args.n_test_step}/{self.args.t_0}")            if self.args.n_test_step != 0:                seq_test = np.linspace(0, 1, self.args.n_test_step) * self.args.t_0                seq_test = [int(s) for s in list(seq_test)]                # print('Uniform skip type')            else:                seq_test = list(range(self.args.t_0))                # print('No skip')            seq_test_next = [-1] + list(seq_test[:-1])            for it in range(self.args.n_iter_gen_my):                if self.args.deterministic_inv:                    x = x_lat.clone()                else:                    e = torch.randn_like(x0)                    a = (1 - self.betas).cumprod(dim=0)                    x = x0 * a[self.args.t_0 - 1].sqrt() + e * (1.0 - a[self.args.t_0 - 1]).sqrt()                tvu.save_image((x + 1) * 0.5, os.path.join(self.args.image_folder,                                                           f'1_lat_ninv{self.args.n_inv_step}.png'))                # with tqdm(total=len(seq_test), desc="Generative process {}".format(it)) as progress_bar:                for i, j in zip(reversed(seq_test), reversed(seq_test_next)):                    t = (torch.ones(n) * i).to(self.device)                    t_next = (torch.ones(n) * j).to(self.device)                    x = denoising_step(x, t=t, t_next=t_next, models=models,                                       logvars=self.logvar,                                       sampling_type=self.args.sample_type,                                       b=self.betas,                                       eta=self.args.eta,                                       learn_sigma=learn_sigma,                                       ratio=self.args.model_ratio,                                       hybrid=self.args.hybrid_noise,                                       hybrid_config=HYBRID_CONFIG)                    # added intermediate step vis                    if (i - 99) % 100 == 0:                        tvu.save_image((x + 1) * 0.5, os.path.join(self.args.image_folder,                                                                   f'2_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}_ngen{self.args.n_test_step}_{i}_it{it}.png'))                        # progress_bar.update(1)                x0 = x.clone()                if self.args.save_name:                    if self.args.lora or self.args.name_of_freeze_function:                        tvu.save_image((x + 1) * 0.5, os.path.join(self.args.exp_dir,                                                                   f"freeze_{self.args.name_of_freeze_function}_lora_r_{self.args.lora_rank}_lora_a_{self.args.lora_alpha}_3_gen_t{self.args.t_0}_it{it}_ninv{self.args.n_inv_step}_ngen{self.args.n_test_step}_mrat{self.args.model_ratio}_{self.args.save_name.split('/')[-1].replace('.pth', '')}.png"))                    else:                        tvu.save_image((x + 1) * 0.5, os.path.join(self.args.exp_dir,                                                               f"3_gen_t{self.args.t_0}_it{it}_ninv{self.args.n_inv_step}_ngen{self.args.n_test_step}_mrat{self.args.model_ratio}_{self.args.save_name.split('/')[-1].replace('.pth','')}.png"))                else:                    if self.args.lora or self.args.name_of_freeze_function:                        tvu.save_image((x + 1) * 0.5, os.path.join(self.args.image_folder,                                                                   f'freeze_{self.args.name_of_freeze_function}_lora_r_{self.args.lora_rank}_lora_a_{self.args.lora_alpha}_3_gen_t{self.args.t_0}_it{it}_ninv{self.args.n_inv_step}_ngen{self.args.n_test_step}_mrat{self.args.model_ratio}.png'))                    else:                        tvu.save_image((x + 1) * 0.5, os.path.join(self.args.image_folder,                                                           f'3_gen_t{self.args.t_0}_it{it}_ninv{self.args.n_inv_step}_ngen{self.args.n_test_step}_mrat{self.args.model_ratio}.png'))    # Computing latent variables    # ----------------------------------------------------------------------------------    def precompute_latents(self):        print("Prepare identity latent")        self.img_lat_pairs_dic = {}        for self.mode in ['train', 'test']:            img_lat_pairs = []            pairs_path = os.path.join('precomputed/',                                      f'{self.config.data.category}_{self.mode}_t{self.args.t_0}_nim{self.args.n_precomp_img}_ninv{self.args.n_inv_step}_pairs.pth')            # Loading latent variables if exists            # -----------------------------------            print(pairs_path)            if os.path.exists(pairs_path):                print(f'{self.mode} pairs exists')                self.img_lat_pairs_dic[self.mode] = torch.load(pairs_path)                for step, (x0, x_id, x_lat) in enumerate(self.img_lat_pairs_dic[self.mode]):                    # self.save(x0, f'{self.mode}_{step}_0_orig.png')                    # self.save(x_id, f'{self.mode}_{step}_1_rec_ninv{self.args.n_inv_step}.png')                    if step == self.args.n_precomp_img - 1:                        break                continue            else:                train_dataset, test_dataset = get_dataset(self.config.data.dataset, DATASET_PATHS, self.config)                loader_dic = get_dataloader(train_dataset, test_dataset, bs_train=self.args.bs_train,                                            num_workers=self.config.data.num_workers)                loader = loader_dic[self.mode]            # -----------------------------------            # Preparation of the latents            # -----------------------------------            n_precomp = 0            with torch.no_grad():                for self.step, img in enumerate(loader):                    if self.args.single_image:                        if self.step != self.args.number_of_image:                            continue                    x0 = img.to(self.config.device)                    if self.args.single_image and self.mode == 'train':                        self.save(x0, f'{self.mode}_{self.step}_0_orig.png')                    x = x0.clone()                    # Inversion of the real image                    x = self.apply_diffusion(x=x,                                             seq_prev=self.seq_inv_next[1:],                                             seq_next=self.seq_inv[1:],                                             is_grad=False, simple=True,                                             process='Inversion process')                    x_lat = x.clone()                    # self.save(x_lat, f'{self.mode}_{self.step}_1_lat_ninv{self.args.n_inv_step}.png')                    # Generation from computed latent variable                    x = self.apply_diffusion(x=x,                                             seq_prev=reversed((self.seq_inv)),                                             seq_next=reversed((self.seq_inv_next)),                                             is_grad=False,                                             is_one_step=True,                                             sample_type=self.args.sample_type,                                             process='Generative process')                    img_lat_pairs.append([x0.detach().cpu(), x.detach().cpu().clone(), x_lat.detach().cpu().clone()])                    # if self.mode == 'test':                    #    self.save(x, f'{self.mode}_{self.step}_1_rec_ninv{self.args.n_inv_step}.png')                    n_precomp += len(x)                    if n_precomp >= self.args.n_precomp_img:                        break                self.img_lat_pairs_dic[self.mode] = img_lat_pairs                pairs_path = os.path.join('precomputed/',                                          f'{self.config.data.category}_{self.mode}_t{self.args.t_0}_nim{self.args.n_precomp_img}_ninv{self.args.n_inv_step}_pairs.pth')                torch.save(img_lat_pairs, pairs_path)            # -----------------------------------    # Fine tune the model    # ----------------------------------------------------------------------------------    def clip_finetune(self):        print(self.args.exp)        print(f'   {self.src_txts}')        print(f'-> {self.trg_txts}')        # set_random_seed(self.args.seed)        # print(type(vars(self.args)))        # print(vars(self.args))        # print(vars(self.args)['wandb_project'])        # print('\n', getattr(vars(self.args), 'wandb_project'))        if self.args.wandb:            self.logger = WanDBWriter(vars(self.args))            current_step = 0            self.logger.set_step(current_step)            total_size = 0            for param in self.model.parameters():                if param.requires_grad:                    total_size += param.numel()            self.logger.add_scalar('total size (%)', total_size / DEFAULT_DDPM_SIZE)            if self.args.wandb_test_my_image:                img = run_alignment('imgs/billie2.png', output_size=self.config.data.image_size)                img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)                self.logger.add_image('photo', img)        self.precompute_latents()        print("Start finetuning")        print(f"Sampling type: {self.args.sample_type.upper()} with eta {self.args.eta}")        # print('checking requires grad finetune')        # for param in self.model.parameters():        #     print(param.requires_grad)        # print('\n\n')        for src_txt, trg_txt in zip(self.src_txts, self.trg_txts):            print(f"CHANGE {src_txt} TO {trg_txt}")            self.clip_loss_func.target_direction = None            wandb_all_time = 0            for it_out in range(self.args.n_iter):                exp_id = os.path.split(self.args.exp)[-1]                save_name = f'checkpoint/{exp_id}_{trg_txt.replace(" ", "_")}-{it_out}.pth'                if self.args.name_of_freeze_function:                    save_name = f'checkpoint/{self.args.name_of_freeze_function}_{exp_id}_{trg_txt.replace(" ", "_")}-{it_out}.pth'                if self.args.lora:                    save_name = 'checkpoint/lora_' + save_name[11:]                self.args.save_name = save_name                if self.args.do_train:                    # print("AAaAAAAAA")                    # Train                    # ----------------------------------------------------                    self.mode = 'train'                    for self.step, (x0, x_id, x_lat) in enumerate(self.img_lat_pairs_dic['train']):                        self.model.train()                        # time_in_start = time.time()                        self.optim_ft.zero_grad()                        x = x_lat.clone().to(self.device)                        wandb_start_forward_time = time.time()                        x = self.apply_diffusion(x=x,                                                 seq_prev=reversed(self.seq_train),                                                 seq_next=reversed(self.seq_train_next),                                                 process=f"CLIP iteration",                                                 sample_type=self.args.sample_type,                                                 is_grad=True,                                                 eta=self.args.eta,                                                 is_one_step=True)                        # Losses                        x_source = x0.to(self.device)                        loss_clip = (2 - self.clip_loss_func(x_source, src_txt, x, trg_txt)) / 2                        loss_clip = -torch.log(loss_clip)                        loss_id = 0                        loss_l1 = nn.L1Loss()(x0.to(self.device), x)                        loss = self.args.clip_loss_w * loss_clip + self.args.id_loss_w * loss_id + self.args.l1_loss_w * loss_l1                        loss.backward()                        wandb_all_time += time.time() - wandb_start_forward_time                        self.optim_ft.step()                        # time_in_end = time.time()                        if self.args.wandb:                            current_step += 1                            self.logger.set_step(current_step)                            self.logger.add_scalar('loss', loss.item())                            self.logger.add_scalar('loss_clip', loss_clip.item())                            # logger.add_scalar('loss_id', loss_id)                            self.logger.add_scalar('loss_l1', loss_l1.item())                            # self.logger.add_scalar('allocated memory (GB)', torch.cuda.memory_allocated(0) / (1024 ** 3))                            self.logger.add_scalar('total time (s)', wandb_all_time)                        # print(f"CLIP {self.step}-{it_out}: loss_l1: {loss_l1:.3f}, loss_clip: {loss_clip:.3f}")                        # print(f"Training for {len(x)} image(s) takes {time_in_end - time_in_start:.4f}s")                        if self.args.single_image:                            # TODO SOMETHING BETTER                            x = x_lat.clone().to(self.device)                            self.model.eval()                            x = self.apply_diffusion(x=x,                                                     seq_prev=reversed(self.seq_train),                                                     seq_next=reversed(self.seq_train_next),                                                     process=f"CLIP iteration",                                                     sample_type=self.args.sample_type,                                                     is_grad=False,                                                     eta=self.args.eta,                                                     is_one_step=False)                            self.save(x,                                      f'train_{self.step}_2_clip_{trg_txt.replace(" ", "_")}_{it_out}_ngen{self.args.n_train_step}.png')                            if self.is_first_train:                                self.save(x0, f'{self.mode}_{self.step}_0_orig.png')                        if self.step == self.args.n_train_img - 1:                            break                    if isinstance(self.model, nn.DataParallel):                        torch.save(self.model.module.state_dict(), save_name)                    else:                        torch.save(self.model.state_dict(), save_name)                        # print('save_name is', save_name)                    self.gen_image('imgs/billie2.png')                    self.scheduler_ft.step()                    self.is_first_train = False                    # ----------------------------------------------------                # Eval                # ----------------------------------------------------                self.mode = 'test'                if self.args.do_test and not self.args.single_image:                    if not self.args.do_train:                        print(save_name)                        self.model.module.load_state_dict(torch.load(save_name))                    self.model.eval()                    for self.step, (x0, x_id, x_lat) in enumerate(self.img_lat_pairs_dic['test']):                        x = self.apply_diffusion(x=x_lat.to(self.device),                                                 seq_prev=reversed(self.seq_train),                                                 seq_next=reversed(self.seq_train_next),                                                 process=f"Eval iteration",                                                 sample_type=self.args.sample_type,                                                 eta=self.args.eta,                                                 is_grad=False,                                                 is_one_step=False)                        if self.is_first:                            self.save(x0, f'{self.mode}_{self.step}_0_orig.png')                        # print(f"Eval {self.step}-{it_out}")                        self.save(x,                                  f'test_{self.step}_2_clip_{trg_txt.replace(" ", "_")}_{it_out}_ngen{self.args.n_test_step}.png')                        if self.step == self.args.n_test_img - 1:                            break                    self.is_first = False                # ----------------------------------------------------    def gen_image(self, img_path):        self.args.img_path = img_path        # print(img_path)        # exp_dir = f"runs/MANI_{img_path.split('/')[-1]}_align{True}"        # exp_dir = self.args.image_folder        exp_dir = f"runs/MANI_{img_path.split('/')[-1]}_align{True}"        self.args.exp_dir = exp_dir        os.makedirs(exp_dir, exist_ok=True)        degree_of_change = 1        # n_iter_forward = 1        self.args.n_iter_gen_my = 1        # n_result = 1        # img_orig = Image.open(os.path.join(exp_dir, '0_orig.png'))        # img_orig = img_orig.resize((int(img_orig.width), int(img_orig.height)))        self.edit_one_image()        # grid = Image.new("RGB", (img_orig.width * (n_result + 1), img_orig.height))        # grid.paste(img_orig, (0, 0))        # for i in range(n_result):        #     img = Image.open(os.path.join(exp_dir,        #         f"3_gen_t{self.args.t_0}_it0_ninv{self.args.n_inv_step}_ngen{self.args.n_test_step}_mrat{degree_of_change}_{self.args.save_name.split('/')[-1].replace('.pth', '')}.png"))        #     img = img.resize((int(img.width), int(img.height)))            # grid.paste(img, (int(img.height * (i + 1)), 0))        if self.args.lora or self.args.name_of_freeze_function:            img = Image.open(os.path.join(self.args.exp_dir,                f"freeze_{self.args.name_of_freeze_function}_lora_r_{self.args.lora_rank}_lora_a_{self.args.lora_alpha}_3_gen_t{self.args.t_0}_it0_ninv{self.args.n_inv_step}_ngen{self.args.n_test_step}_mrat{degree_of_change}_{self.args.save_name.split('/')[-1].replace('.pth', '')}.png"))        else:            img = Image.open(os.path.join(self.args.exp_dir,                f"3_gen_t{self.args.t_0}_it0_ninv{self.args.n_inv_step}_ngen{self.args.n_test_step}_mrat{degree_of_change}_{self.args.save_name.split('/')[-1].replace('.pth', '')}.png"))        img = img.resize((int(img.width), int(img.height)))        if self.args.wandb_test_my_image:            # if self.is_first_train:            #     img = Image.open(self.args.img_path).convert("RGB")            #     self.logger.add_image('photo', img)            self.logger.add_image('photo', img)    ####################################################################################    # UTILS FUNCTIONS    # Preparation of sequences    # ----------------------------------------------------------------------------------    def _conf_seqs(self):        seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        self.seq_inv = [int(s) for s in list(seq_inv)]        self.seq_inv_next = [-1] + list(self.seq_inv[:-1])        if self.args.n_train_step != 0:            seq_train = np.linspace(0, 1, self.args.n_train_step) * self.args.t_0            self.seq_train = [int(s) for s in list(seq_train)]            print('Uniform skip type')        else:            self.seq_train = list(range(self.args.t_0))            print('No skip')        self.seq_train_next = [-1] + list(self.seq_train[:-1])        self.seq_test = np.linspace(0, 1, self.args.n_test_step) * self.args.t_0        self.seq_test = [int(s) for s in list(self.seq_test)]        self.seq_test_next = [-1] + list(self.seq_test[:-1])    # ----------------------------------------------------------------------------------    # Slava's        def set_model(self, model_path):        self.args.model_path = model_path        if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:            model = DDPM(self.config)            if self.args.model_path:                init_ckpt = torch.load(model_path)            # else:            #     init_ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)            self.learn_sigma = False            print("Original diffusion Model loaded.")        elif self.config.data.dataset in ["FFHQ", "AFHQ", "IMAGENET"]:            model = i_DDPM(self.config.data.dataset)            if model_path:                init_ckpt = torch.load(model_path)            else:                init_ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])            self.learn_sigma = True            print("Improved diffusion Model loaded.")        else:            print('Not implemented dataset')            raise ValueError        model.load_state_dict(init_ckpt)        model.to(self.device)        self.model = model        # ----------------------------------------------------------------------------------    # Slava's        def set_model_by_model(self, model, style='down'):        # self.args.model_path = self.args.model_path.model_path[:-4] + 'freeze' + style + '.pth'        print('new path is', self.args.model_path)        model.to(self.device)        self.model = model                    # Configuration of the diffusion model    # ----------------------------------------------------------------------------------    def _conf_model(self):        if self.config.data.dataset == "LSUN":            if self.config.data.category == "bedroom":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt"            elif self.config.data.category == "church_outdoor":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt"        elif self.config.data.dataset == "CelebA_HQ":            url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt"        elif self.config.data.dataset == "AFHQ":            pass        elif self.config.data.dataset == "IMAGENET":            pass        else:            raise ValueError        if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:            if self.args.lora:                model = DDPM_lora(self.config, self.args.lora_rank, self.args.lora_alpha)            else:                model = DDPM(self.config)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)            self.learn_sigma = False            print("Original diffusion Model loaded.")        elif self.config.data.dataset in ["FFHQ", "AFHQ", "IMAGENET"]:            model = i_DDPM(self.config.data.dataset)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])            self.learn_sigma = True            print("Improved diffusion Model loaded.")        else:            print('Not implemented dataset')            raise ValueError        if self.args.lora:            load_weight_lora(model, init_ckpt)        else:            model.load_state_dict(init_ckpt)        if self.args.name_of_freeze_function:            if self.args.name_of_freeze_function not in freeze_function_mapping:                print('wrong name of the freeze function! \t skipping this step')                raise ValueError            else:                freeze_function_mapping[self.args.name_of_freeze_function](model)        if self.args.lora:            lora.mark_only_lora_as_trainable(model, bias='lora_only')        model.to(self.device)        self.model = model        # print('checking requires grad conf')        # for param in self.model.parameters():        #     print(param.requires_grad)        # print('\n\n')    # ----------------------------------------------------------------------------------    # Configuration of the optimizer    # ----------------------------------------------------------------------------------    def _conf_opt(self):        print(f"Setting optimizer with lr={self.args.lr_clip_finetune}")        params_to_update = []        for name, param in self.model.named_parameters():            if param.requires_grad == True:                params_to_update.append(param)                # print(name)        self.optim_ft = torch.optim.Adam(params_to_update, weight_decay=0, lr=self.args.lr_clip_finetune)        self.init_opt_ckpt = self.optim_ft.state_dict()        self.scheduler_ft = torch.optim.lr_scheduler.StepLR(self.optim_ft, step_size=1, gamma=self.args.sch_gamma)        self.init_sch_ckpt = self.scheduler_ft.state_dict()    # Configuration of the loss    # ----------------------------------------------------------------------------------    def _conf_loss(self):        print("Loading losses")        self.clip_loss_func = CLIPLoss(            self.device,            lambda_direction=1,            lambda_patch=0,            lambda_global=0,            lambda_manifold=0,            lambda_texture=0,            clip_model=self.args.clip_model_name)        self.id_loss_func = id_loss.IDLoss().to(self.device).eval()    # ----------------------------------------------------------------------------------    @torch.no_grad()    def save(self, x, name):        tvu.save_image((x + 1) * 0.5, os.path.join(self.args.image_folder, name))    ####################################################################################